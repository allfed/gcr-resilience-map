{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PDF Information Extractor v2\n",
                "\n",
                "This notebook extracts information from PDF papers about regional resilience to catastrophic risks using Claude. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import required libraries\n",
                "import os\n",
                "from pathlib import Path\n",
                "import PyPDF2\n",
                "import anthropic\n",
                "import pandas as pd\n",
                "from tqdm import tqdm\n",
                "import tiktoken\n",
                "import csv\n",
                "import json\n",
                "from io import StringIO\n",
                "import hashlib\n",
                "import datetime\n",
                "import time as sleep_time"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup Anthropic Client\n",
                "# read the API key from the file    \n",
                "with open(\"../config/api_key.txt\", 'r') as f:\n",
                "    api_key = f.read().strip()\n",
                "\n",
                "# Initialize Anthropic client\n",
                "client = anthropic.Anthropic(api_key=api_key)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "## PDF Processing Functions\n",
                "def extract_text_from_pdf(pdf_path: str) -> str:\n",
                "    \"\"\"Extract text from a PDF file\"\"\"\n",
                "    with open(pdf_path, 'rb') as file:\n",
                "        reader = PyPDF2.PdfReader(file)\n",
                "        text = \"\"\n",
                "        for page in reader.pages:\n",
                "            text += page.extract_text() + \"\\n\"\n",
                "    return text\n",
                "\n",
                "\n",
                "def process_with_claude(text: str, query: str, temperature: float = 0, max_tokens: int = 1000) -> str:\n",
                "    \"\"\"Process text with Claude model using prefix caching\"\"\"\n",
                "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
                "    token_count = len(encoding.encode(text))\n",
                "    token_count = int(token_count * 1.1)\n",
                "    print(f\"Token count is approximately {token_count}\")\n",
                "\n",
                "    response = client.messages.create(\n",
                "        model=\"claude-3-7-sonnet-20250219\",\n",
                "        max_tokens=max_tokens,\n",
                "        temperature=temperature,\n",
                "        system=[\n",
                "            {\n",
                "                \"type\": \"text\",\n",
                "                \"text\": \"You are an AI assistant tasked with analyzing documents.\",\n",
                "            }\n",
                "        ],\n",
                "        messages=[\n",
                "            {\n",
                "                \"role\": \"user\",\n",
                "                \"content\": f\"Document content:\\n{text}\\n\\n{query}\"\n",
                "            }\n",
                "        ]\n",
                "    )\n",
                "\n",
                "    # Log  information if available\n",
                "    if hasattr(response, 'usage'):\n",
                "        print(f\"API Response Token Usage:\")\n",
                "        print(f\"  - Total input tokens: {response.usage.input_tokens}\")\n",
                "        \n",
                "    return response.content[0].text\n",
                "\n",
                "\n",
                "def parse_csv_response(response_text, columns):\n",
                "    \"\"\"Parse the CSV response from Claude and return a dictionary with column names as keys\"\"\"\n",
                "    # Clean the response text\n",
                "    clean_text = response_text.strip()\n",
                "\n",
                "    # If there are multiple lines, take only the CSV line\n",
                "    if \"\\n\" in clean_text:\n",
                "        # Find the line that has the most commas (likely the CSV data)\n",
                "        lines = clean_text.split('\\n')\n",
                "        clean_text = max(lines, key=lambda x: x.count(','))\n",
                "\n",
                "    # Parse CSV using the csv module which handles quoted fields properly\n",
                "    reader = csv.reader(StringIO(clean_text))\n",
                "    try:\n",
                "        row = next(reader)\n",
                "        # Map values to column names\n",
                "        result = {col: val for col, val in zip(columns, row)}\n",
                "        return result\n",
                "    except StopIteration:\n",
                "        # If parsing fails, return the original text\n",
                "        return {\"error\": \"Failed to parse CSV response\", \"original_text\": clean_text}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Extraction prompt and configuration\n",
                "extraction_query = \"\"\"I need you to analyze the provided research paper and extract specific information about regional resilience to catastrophic risks. Our research question is: \"What specific geographical, institutional, and infrastructural factors have been empirically or theoretically identified as enhancing regional resilience to nuclear winter, large magnitude volcanic eruptions, extreme pandemics, and infrastructure collapse catastrophes, and how do these resilience factors vary across catastrophe types?\"\n",
                "\n",
                "After analyzing the paper thoroughly, provide your output in a single row CSV format with the following structure:\n",
                "\n",
                "1. paper_citation: Full citation (author, year, title)\n",
                "2. publication_type: [Journal article/Preprint/Report/Book chapter]\n",
                "3. gcr_types: Types of catastrophic risks addressed [Nuclear/Volcanic/Asteroid/Infrastructure/Pandemic/Climate/Multiple]\n",
                "4. geographic_focus: [Global/Regional/National/Local/Islands - specify]\n",
                "5. geographic_factors: List key geographic factors (location, climate, resources, etc.)\n",
                "6. institutional_factors: List key institutional factors (governance, policies, social systems, etc.)\n",
                "7. infrastructural_factors: List key infrastructure factors (energy, food, communications, etc.)\n",
                "8. other_resilience_factors: Any resilience factors not fitting above categories\n",
                "9. study_approach: [Model/Empirical/Review/Case study/Theoretical]\n",
                "10. resilience_phase: [Preparedness/Robustness/Recovery/Adaptation]\n",
                "11. main_resilience_factors: Brief summary of main resilience-enhancing factors\n",
                "12. resilience_tradeoffs: [Yes/No] with description of any identified trade-offs\n",
                "13. vulnerable_resilient_regions: List of particularly vulnerable or resilient regions identified\n",
                "14. overall_relevance: [Low/Medium/High] relevance to our research question\n",
                "15. evidence_gaps: Brief description of critical missing validation elements\n",
                "\n",
                "For text fields, place the content in double quotes to properly handle any commas. For fields with multiple options, use the exact values specified in brackets. Please analyze the paper thoroughly before extracting the information. Respond with ONLY the CSV row (no column headers).\"\"\"\n",
                "\n",
                "# Define the column names based on the structure\n",
                "extraction_columns = [\n",
                "    \"paper_citation\", \"publication_type\", \"gcr_types\", \"geographic_focus\",\n",
                "    \"geographic_factors\", \"institutional_factors\", \"infrastructural_factors\",\n",
                "    \"other_resilience_factors\", \"study_approach\", \"resilience_phase\",\n",
                "    \"main_resilience_factors\", \"resilience_tradeoffs\", \"vulnerable_resilient_regions\",\n",
                "    \"overall_relevance\", \"evidence_gaps\"\n",
                "]\n",
                "\n",
                "# Process configuration\n",
                "pdf_dir = \"pdf\"  # Directory containing PDF files\n",
                "temperature = 0   # Keep it as deterministic as possible\n",
                "max_tokens = 4000  # Increased token limit for more detailed responses\n",
                "cache_dir = \"prompt_cache\"  # Directory for caching prompts and responses\n",
                "\n",
                "# Define configuration for extraction process\n",
                "config = {\n",
                "    'output_file': \"gcr_resilience_extraction_results.csv\",\n",
                "    'cache_file': os.path.join(cache_dir, \"extraction_prompt_cache.json\"),\n",
                "    'query': extraction_query,\n",
                "    'columns': extraction_columns\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded 1 cached responses\n"
                    ]
                }
            ],
            "source": [
                "# Create directories if they don't exist\n",
                "os.makedirs(pdf_dir, exist_ok=True)\n",
                "os.makedirs(cache_dir, exist_ok=True)\n",
                "\n",
                "# Initialize cache and results\n",
                "cache = {}\n",
                "results = []\n",
                "processed_files = set()\n",
                "\n",
                "# Load cache and existing results\n",
                "cache_file = config['cache_file']\n",
                "if os.path.exists(cache_file):\n",
                "    try:\n",
                "        with open(cache_file, 'r') as f:\n",
                "            cache = json.load(f)\n",
                "        print(f\"Loaded {len(cache)} cached responses\")\n",
                "    except Exception as e:\n",
                "        print(f\"Error loading cache: {str(e)}\")\n",
                "else:\n",
                "    # Create empty cache file if it doesn't exist\n",
                "    with open(cache_file, 'w') as f:\n",
                "        json.dump({}, f)\n",
                "    print(\"Created new empty cache file\")\n",
                "\n",
                "# Load results\n",
                "output_file = config['output_file']\n",
                "if os.path.exists(output_file):\n",
                "    try:\n",
                "        df = pd.read_csv(output_file)\n",
                "        results = df.to_dict('records')\n",
                "        processed_files = set(df['filename'].tolist())\n",
                "        print(f\"Loaded {len(results)} existing results\")\n",
                "    except Exception as e:\n",
                "        print(f\"Error loading existing results: {str(e)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'441c5f990d6204a5ae8ef2f737e9ce14': '\"Moersdorf, J., Rivers, M., Denkenberger, D., Breuer, L., & Jehn, F. U. (2024). The Fragile State of Industrial Agriculture: Estimating Crop Yield Reduction in a Global Catastrophic Infrastructure Loss Scenario. Global Challenges, 8(1), 2300206.\",\"Journal article\",\"Infrastructure\",\"Global\",\"Climate conditions (thermal regime, moisture regime, soil conditions); Agricultural land availability; Natural resource distribution\",\"Preparedness and response plans; International cooperation; Trade networks; Food system policies; Agricultural knowledge preservation\",\"Electrical grid resilience; Agricultural mechanization; Irrigation systems; Fertilizer production and distribution; Pesticide availability; Transportation networks; Seed storage and distribution\",\"Crop diversity; Farming techniques (traditional vs. industrial); Nitrogen-fixing capabilities of certain crops (e.g., soybeans)\",\"Model\",\"Preparedness\",\"Diversification of agricultural systems; Reduced dependence on industrial inputs; Maintenance of traditional farming knowledge; Resilient food alternatives; Robust electrical infrastructure; Decentralized food production\",\"Yes, trade-offs between resilience strategies: smallholder farming may be less resilient to local disruptions; organic farming requires more land; permaculture demands more manual labor\",\"Vulnerable: Central Europe, North America, South America, parts of India, China, and Indonesia (up to 75% yield reduction); Resilient: Most African countries (less affected due to less industrialized agriculture)\",\"High\",\"Limited data on seed availability and crop varieties; Lack of information on farmers\\' adaptability; Insufficient understanding of economic system responses; Limited analysis of alternative food production methods; Inadequate assessment of communication systems for coordination\"'}"
                        ]
                    },
                    "execution_count": 17,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "cache"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Processing PDFs:   0%|          | 0/22 [00:00<?, ?it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Processing Moersdorf et al. - 2024 - The Fragile State of Industrial Agriculture Estimating Crop Yield Reductions in a Global Catastroph.pdf...\n",
                        "Extracted 82210 characters from PDF.\n",
                        "Token count is approximately 26273\n",
                        "API Response Token Usage:\n",
                        "  - Total input tokens: 30207\n",
                        "Received response of length 1824\n",
                        "Sleeping for 60 seconds to avoid rate limits...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Processing PDFs:   0%|          | 0/22 [00:22<?, ?it/s]\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[13], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# Sleep for 1 minute to avoid rate limits\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSleeping for 60 seconds to avoid rate limits...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m     sleep_time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Parse the CSV response\u001b[39;00m\n\u001b[1;32m     37\u001b[0m parsed_result \u001b[38;5;241m=\u001b[39m parse_csv_response(response, config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "# Process PDFs\n",
                "pdf_files = list(Path(pdf_dir).glob('*.pdf'))\n",
                "\n",
                "for pdf_path in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
                "    if pdf_path.name not in processed_files:\n",
                "        try:\n",
                "            print(f\"Processing {pdf_path.name}...\")\n",
                "            \n",
                "            # Extract text\n",
                "            text = extract_text_from_pdf(str(pdf_path))\n",
                "            print(f\"Extracted {len(text)} characters from PDF.\")\n",
                "            \n",
                "            # Create cache key\n",
                "            cache_key = hashlib.md5((text + config['query']).encode()).hexdigest()\n",
                "            \n",
                "            # Check for cached response\n",
                "            if cache_key in cache:\n",
                "                print(f\"Using cached response for {pdf_path.name}\")\n",
                "                response = cache[cache_key]\n",
                "            else:\n",
                "                # Process with Claude\n",
                "                response = process_with_claude(text, config['query'], temperature, max_tokens)\n",
                "                print(f\"Received response of length {len(response)}\")\n",
                "                \n",
                "                # Cache the response\n",
                "                cache[cache_key] = response\n",
                "                \n",
                "                # Save cache after each new response\n",
                "                with open(config['cache_file'], 'w') as f:\n",
                "                    json.dump(cache, f)\n",
                "                \n",
                "                # Sleep for 1 minute to avoid rate limits\n",
                "                print(f\"Sleeping for 60 seconds to avoid rate limits...\")\n",
                "                sleep_time.sleep(60)\n",
                "            \n",
                "            # Parse the CSV response\n",
                "            parsed_result = parse_csv_response(response, config['columns'])\n",
                "            \n",
                "            # Add the filename for reference\n",
                "            parsed_result['filename'] = pdf_path.name\n",
                "            \n",
                "            # Add to results\n",
                "            results.append(parsed_result)\n",
                "            \n",
                "            # Save intermediate results\n",
                "            interim_df = pd.DataFrame(results)\n",
                "            interim_df.to_csv(config['output_file'], index=False)\n",
                "            \n",
                "            print(f\"Successfully processed {pdf_path.name}\")\n",
                "        \n",
                "        except Exception as e:\n",
                "            print(f\"Error processing {pdf_path.name}: {str(e)}\")\n",
                "            # Add error record\n",
                "            error_result = {\n",
                "                \"error\": str(e),\n",
                "                \"filename\": pdf_path.name\n",
                "            }\n",
                "            results.append(error_result)\n",
                "            \n",
                "            # Save intermediate results even after errors\n",
                "            interim_df = pd.DataFrame(results)\n",
                "            interim_df.to_csv(config['output_file'], index=False)\n",
                "    else:\n",
                "        print(f\"Skipping already processed file: {pdf_path.name}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create final DataFrame from the results\n",
                "extraction_df = pd.DataFrame(results)\n",
                "\n",
                "# Add timestamp for versioned output\n",
                "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
                "versioned_extraction_csv = f\"{os.path.splitext(config['output_file'])[0]}_{timestamp}.csv\"\n",
                "\n",
                "# Save versioned output\n",
                "extraction_df.to_csv(config['output_file'], index=False)\n",
                "extraction_df.to_csv(versioned_extraction_csv, index=False)\n",
                "\n",
                "print(f\"Extraction results saved to {config['output_file']} and {versioned_extraction_csv}\")\n",
                "\n",
                "# Display results\n",
                "print(\"Extraction results:\")\n",
                "print(extraction_df.head())"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
