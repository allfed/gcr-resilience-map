{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Processing System\n",
    "\n",
    "This notebook processes academic papers to extract information about regional resilience to catastrophic risks using Claude.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's import all required libraries and set up logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from pathlib import Path\n",
    "import PyPDF2\n",
    "import anthropic\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "import csv\n",
    "import json\n",
    "from io import StringIO\n",
    "import hashlib\n",
    "import datetime\n",
    "import time as sleep_time\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Dict, Any, Set  # Added List and other type hints\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structures and Classes\n",
    "\n",
    "Define the data structures and classes needed for processing papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PaperMetadata:\n",
    "    \"\"\"Data class for paper metadata\"\"\"\n",
    "    filename: str\n",
    "    paper_citation: Optional[str] = None\n",
    "    publication_type: Optional[str] = None\n",
    "    gcr_types: Optional[str] = None\n",
    "    geographic_focus: Optional[str] = None\n",
    "    geographic_factors: Optional[str] = None\n",
    "    institutional_factors: Optional[str] = None\n",
    "    infrastructural_factors: Optional[str] = None\n",
    "    other_resilience_factors: Optional[str] = None\n",
    "    study_approach: Optional[str] = None\n",
    "    resilience_phase: Optional[str] = None\n",
    "    main_resilience_factors: Optional[str] = None\n",
    "    resilience_tradeoffs: Optional[str] = None\n",
    "    vulnerable_resilient_regions: Optional[str] = None\n",
    "    overall_relevance: Optional[str] = None\n",
    "    evidence_gaps: Optional[str] = None\n",
    "    error: Optional[str] = None\n",
    "    current_query: Optional[str] = None  # Added for token calculations\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert to dictionary for storage, excluding current_query\"\"\"\n",
    "        return {k: v for k, v in self.__dict__.items()\n",
    "                if v is not None and k != 'current_query'}  # Exclude current_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDFExtractor:\n",
    "    \"\"\"Extracts text from PDF files\"\"\"\n",
    "    def extract(self, file_path: Path) -> str:\n",
    "        \"\"\"Extract text from a PDF file\"\"\"\n",
    "        with open(file_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "        return text\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"Handles text tokenization and truncation\"\"\"\n",
    "    def __init__(self, encoding_name: str = \"cl100k_base\"):\n",
    "        self.encoding = tiktoken.get_encoding(encoding_name)\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Count tokens in text\"\"\"\n",
    "        return len(self.encoding.encode(text))\n",
    "    \n",
    "    def truncate(self, text: str, ratio: float) -> str:\n",
    "        \"\"\"Truncate text to a given ratio of its original length\"\"\"\n",
    "        tokens = self.encoding.encode(text)\n",
    "        return self.encoding.decode(tokens[:int(len(tokens) * ratio)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClaudeClient:\n",
    "    \"\"\"Client for Claude API\"\"\"\n",
    "    def __init__(self, api_key: str, model: str = \"claude-3-7-sonnet-20250219\"):\n",
    "        self.client = anthropic.Anthropic(api_key=api_key)\n",
    "        self.model = model\n",
    "    \n",
    "    def process_text(self, text: str, query: str, **kwargs) -> str:\n",
    "        \"\"\"Process text with Claude\"\"\"\n",
    "        response = self.client.messages.create(\n",
    "            model=self.model,\n",
    "            max_tokens=kwargs.get('max_tokens', 1000),\n",
    "            temperature=kwargs.get('temperature', 0),\n",
    "            system=[{\"type\": \"text\", \"text\": \"You are an AI assistant tasked with analyzing documents.\"}],\n",
    "            messages=[{\"role\": \"user\", \"content\": f\"Document content:\\n{text}\\n\\n{query}\"}]\n",
    "        )\n",
    "        return response.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileCache:\n",
    "    \"\"\"Simple file-based cache\"\"\"\n",
    "    def __init__(self, cache_file: Path):\n",
    "        self.cache_file = cache_file\n",
    "        self.cache: Dict[str, str] = {}\n",
    "        self._load_cache()\n",
    "    \n",
    "    def _load_cache(self) -> None:\n",
    "        \"\"\"Load cache from file\"\"\"\n",
    "        if self.cache_file.exists():\n",
    "            with open(self.cache_file, 'r') as f:\n",
    "                self.cache = json.load(f)\n",
    "    \n",
    "    def _save_cache(self) -> None:\n",
    "        \"\"\"Save cache to file\"\"\"\n",
    "        with open(self.cache_file, 'w') as f:\n",
    "            json.dump(self.cache, f)\n",
    "    \n",
    "    def get(self, key: str) -> Optional[str]:\n",
    "        \"\"\"Get cached response\"\"\"\n",
    "        return self.cache.get(key)\n",
    "    \n",
    "    def set(self, key: str, value: str) -> None:\n",
    "        \"\"\"Cache a response\"\"\"\n",
    "        self.cache[key] = value\n",
    "        self._save_cache()\n",
    "\n",
    "\n",
    "class CSVStorage:\n",
    "    \"\"\"Handles storage of results in CSV format\"\"\"\n",
    "\n",
    "    def __init__(self, output_file: Path):\n",
    "        self.output_file = output_file\n",
    "        self._ensure_file_exists()\n",
    "\n",
    "    def _ensure_file_exists(self):\n",
    "        if not self.output_file.exists():\n",
    "            # Create empty file with headers\n",
    "            df = pd.DataFrame(columns=PaperMetadata.__annotations__.keys())\n",
    "            df.to_csv(self.output_file, index=False)\n",
    "\n",
    "    def is_processed(self, filename: str) -> bool:\n",
    "        \"\"\"Check if a file has already been processed\"\"\"\n",
    "        try:\n",
    "            if not self.output_file.exists():\n",
    "                return False\n",
    "            df = pd.read_csv(self.output_file)\n",
    "            return filename in df['filename'].values\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error checking if file is processed: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def get_processed_files(self) -> set:\n",
    "        \"\"\"Get set of all processed filenames\"\"\"\n",
    "        try:\n",
    "            if not self.output_file.exists():\n",
    "                return set()\n",
    "            df = pd.read_csv(self.output_file)\n",
    "            return set(df['filename'].values)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting processed files: {str(e)}\")\n",
    "            return set()\n",
    "\n",
    "    def save_results(self, results: List[PaperMetadata]):\n",
    "        \"\"\"Save results to CSV file\"\"\"\n",
    "        try:\n",
    "            df = pd.DataFrame([r.to_dict() for r in results])\n",
    "            df.to_csv(self.output_file, index=False)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving results: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperProcessor:\n",
    "    \"\"\"Main paper processing class\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_extractor: PDFExtractor,\n",
    "        tokenizer: Tokenizer,\n",
    "        llm_client: ClaudeClient,\n",
    "        cache: FileCache,\n",
    "        storage: CSVStorage,\n",
    "        max_tokens: int = 4000,\n",
    "        truncation_ratio: float = 0.8,\n",
    "        tokens_per_minute: int = 20000,  # Claude's rate limit\n",
    "        max_total_tokens: int = 200000   # Claude's total token limit\n",
    "    ):\n",
    "        self.text_extractor = text_extractor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.llm_client = llm_client\n",
    "        self.cache = cache\n",
    "        self.storage = storage\n",
    "        self.max_tokens = max_tokens\n",
    "        self.truncation_ratio = truncation_ratio\n",
    "        self.tokens_per_minute = tokens_per_minute\n",
    "        self.max_total_tokens = max_total_tokens\n",
    "\n",
    "        # Rate limiting tracking\n",
    "        self.token_usage = []  # List of (timestamp, token_count) tuples\n",
    "\n",
    "        # Calculate prompt tokens once\n",
    "        self.prompt_tokens = self.tokenizer.count_tokens(\n",
    "            \"Document content:\\n\\n\" +  # Base prompt\n",
    "            \"You are an AI assistant tasked with analyzing documents.\"  # System prompt\n",
    "        )\n",
    "\n",
    "    def _calculate_max_paper_tokens(self) -> int:\n",
    "        \"\"\"Calculate maximum tokens allowed for the paper text\"\"\"\n",
    "        # Reserve space for the query and other prompt elements\n",
    "        query_tokens = self.tokenizer.count_tokens(self.current_query)\n",
    "        reserved_tokens = self.prompt_tokens + query_tokens + 1000  # Add buffer\n",
    "        return self.max_total_tokens - reserved_tokens\n",
    "\n",
    "    def _cleanup_old_usage(self):\n",
    "        \"\"\"Remove token usage records older than 1 minute\"\"\"\n",
    "        current_time = datetime.datetime.now()\n",
    "        one_minute_ago = current_time - datetime.timedelta(minutes=1)\n",
    "        self.token_usage = [(ts, count) for ts, count in self.token_usage if ts > one_minute_ago]\n",
    "\n",
    "    def _get_current_token_usage(self) -> int:\n",
    "        \"\"\"Get total token usage in the last minute\"\"\"\n",
    "        self._cleanup_old_usage()\n",
    "        return sum(count for _, count in self.token_usage)\n",
    "\n",
    "    def _wait_for_rate_limit(self, required_tokens: int):\n",
    "        \"\"\"Wait if necessary to stay within rate limits\"\"\"\n",
    "        while True:\n",
    "            current_usage = self._get_current_token_usage()\n",
    "            if current_usage + required_tokens <= self.tokens_per_minute:\n",
    "                break\n",
    "            \n",
    "            # Check if we have any usage records\n",
    "            if not self.token_usage:\n",
    "                # If no records, we can proceed\n",
    "                break\n",
    "                \n",
    "            # Calculate how long to wait\n",
    "            oldest_record = min(ts for ts, _ in self.token_usage)\n",
    "            wait_time = 60 - (datetime.datetime.now() - oldest_record).total_seconds()\n",
    "            if wait_time > 0:\n",
    "                logger.info(f\"Rate limit reached. Waiting {wait_time:.1f} seconds...\")\n",
    "                sleep_time.sleep(wait_time)\n",
    "            self._cleanup_old_usage()\n",
    "\n",
    "    def _record_token_usage(self, token_count: int):\n",
    "        \"\"\"Record token usage for rate limiting\"\"\"\n",
    "        self.token_usage.append((datetime.datetime.now(), token_count))\n",
    "\n",
    "    def process_paper(self, file_path: Path, query: str) -> PaperMetadata:\n",
    "        \"\"\"Process a single paper\"\"\"\n",
    "        # Store query for token calculations\n",
    "        self.current_query = query\n",
    "\n",
    "        # Extract text\n",
    "        text = self.text_extractor.extract(file_path)\n",
    "        logger.info(f\"Extracted {len(text)} characters from {file_path.name}\")\n",
    "\n",
    "        # Check cache\n",
    "        cache_key = hashlib.md5((text + query).encode()).hexdigest()\n",
    "        if cached_response := self.cache.get(cache_key):\n",
    "            logger.info(f\"Using cached response for {file_path.name}\")\n",
    "            return self._parse_response(cached_response, file_path.name)\n",
    "\n",
    "        # Process with LLM\n",
    "        current_text = text\n",
    "        while True:\n",
    "            try:\n",
    "                # Check rate limit and wait if necessary\n",
    "                total_tokens = self.tokenizer.count_tokens(current_text) + self.prompt_tokens + self.tokenizer.count_tokens(query)\n",
    "                self._wait_for_rate_limit(total_tokens)\n",
    "                \n",
    "                # Try to process\n",
    "                response = self.llm_client.process_text(current_text, query)\n",
    "                self._record_token_usage(total_tokens)\n",
    "                self.cache.set(cache_key, response)\n",
    "                return self._parse_response(response, file_path.name)\n",
    "                \n",
    "            except Exception as e:\n",
    "                if \"too long\" in str(e).lower():\n",
    "                    # Truncate to 80% of current length\n",
    "                    current_tokens = self.tokenizer.count_tokens(current_text)\n",
    "                    target_tokens = int(current_tokens * self.truncation_ratio)\n",
    "                    target_tokens = max(1000, target_tokens)  # Ensure minimum length\n",
    "                    \n",
    "                    logger.info(f\"Token limit exceeded. Truncating from {current_tokens} to {target_tokens} tokens...\")\n",
    "                    tokens = self.tokenizer.encoding.encode(current_text)\n",
    "                    current_text = self.tokenizer.encoding.decode(tokens[:target_tokens])\n",
    "                else:\n",
    "                    raise e\n",
    "                \n",
    "    def _parse_response(self, response: str, filename: str) -> PaperMetadata:\n",
    "        \"\"\"Parse CSV response into PaperMetadata object\"\"\"\n",
    "        try:\n",
    "            # Clean the response text\n",
    "            clean_text = response.strip()\n",
    "            \n",
    "            # Find the first line that looks like a CSV row (has multiple commas)\n",
    "            # and doesn't contain the prompt text\n",
    "            lines = clean_text.split('\\n')\n",
    "            csv_line = None\n",
    "            for line in lines:\n",
    "                # Skip lines that contain parts of the prompt\n",
    "                if \"research question\" in line.lower() or \"csv format\" in line.lower():\n",
    "                    continue\n",
    "                # Find the line with the most commas that's not the prompt\n",
    "                if line.count(',') >= 10:  # We expect at least 10 commas for our CSV format\n",
    "                    csv_line = line\n",
    "                    break\n",
    "            \n",
    "            if not csv_line:\n",
    "                raise ValueError(\"No valid CSV line found in response\")\n",
    "            \n",
    "            # Parse CSV\n",
    "            reader = csv.reader(StringIO(csv_line))\n",
    "            row = next(reader)\n",
    "            \n",
    "            # Create PaperMetadata object with all fields\n",
    "            return PaperMetadata(\n",
    "                filename=filename,\n",
    "                paper_citation=row[0] if len(row) > 0 else None,\n",
    "                publication_type=row[1] if len(row) > 1 else None,\n",
    "                gcr_types=row[2] if len(row) > 2 else None,\n",
    "                geographic_focus=row[3] if len(row) > 3 else None,\n",
    "                geographic_factors=row[4] if len(row) > 4 else None,\n",
    "                institutional_factors=row[5] if len(row) > 5 else None,\n",
    "                infrastructural_factors=row[6] if len(row) > 6 else None,\n",
    "                other_resilience_factors=row[7] if len(row) > 7 else None,\n",
    "                study_approach=row[8] if len(row) > 8 else None,\n",
    "                resilience_phase=row[9] if len(row) > 9 else None,\n",
    "                main_resilience_factors=row[10] if len(row) > 10 else None,\n",
    "                resilience_tradeoffs=row[11] if len(row) > 11 else None,\n",
    "                vulnerable_resilient_regions=row[12] if len(row) > 12 else None,\n",
    "                overall_relevance=row[13] if len(row) > 13 else None,\n",
    "                evidence_gaps=row[14] if len(row) > 14 else None,\n",
    "                current_query=self.current_query\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing response for {filename}: {str(e)}\")\n",
    "            return PaperMetadata(filename=filename, error=str(e))\n",
    "                \n",
    "    def process_directory(self, directory: Path, query: str) -> None:\n",
    "        \"\"\"Process all PDF files in a directory\"\"\"\n",
    "        results = []\n",
    "        pdf_files = list(directory.glob(\"*.pdf\"))\n",
    "        \n",
    "        for pdf_path in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "            try:\n",
    "                # Skip if already processed\n",
    "                if self.storage.is_processed(pdf_path.name):\n",
    "                    logger.info(f\"Loading cached results for: {pdf_path.name}\")\n",
    "                    # Extract text to generate same cache key\n",
    "                    text = self.text_extractor.extract(pdf_path)\n",
    "                    cache_key = hashlib.md5((text + query).encode()).hexdigest()\n",
    "    \n",
    "                    # Get cached response\n",
    "                    if cached_response := self.cache.get(cache_key):\n",
    "                        result = self._parse_response(cached_response, pdf_path.name)\n",
    "                        results.append(result)\n",
    "                    continue\n",
    "                else:\n",
    "                    logger.warning(f\"File {pdf_path.name} marked as processed but no cache found, reprocessing...\")\n",
    "\n",
    "                logger.info(f\"Processing {pdf_path.name}...\")\n",
    "                result = self.process_paper(pdf_path, query)\n",
    "                results.append(result)\n",
    "\n",
    "                # Save after each successful processing\n",
    "                self.storage.save_results(results)\n",
    "                logger.info(f\"Successfully processed {pdf_path.name}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {pdf_path.name}: {str(e)}\")\n",
    "                # Add error record\n",
    "                error_result = PaperMetadata(\n",
    "                    filename=pdf_path.name,\n",
    "                    error=str(e)\n",
    "                )\n",
    "                results.append(error_result)\n",
    "                # Save even after errors\n",
    "                self.storage.save_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up the configuration and initialize components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key\n",
    "with open(\"../config/api_key.txt\", 'r') as f:\n",
    "    api_key = f.read().strip()\n",
    "\n",
    "# Define extraction query\n",
    "query = \"\"\"I need you to analyze the provided research paper and extract specific information about regional resilience to catastrophic risks. Our research question is: \"What specific geographical, institutional, and infrastructural factors have been empirically or theoretically identified as enhancing regional resilience to nuclear winter, large magnitude volcanic eruptions, extreme pandemics, and infrastructure collapse catastrophes, and how do these resilience factors vary across catastrophe types?\"\n",
    "\n",
    "After analyzing the paper thoroughly, provide your output in a single row CSV format with the following structure:\n",
    "\n",
    "1. paper_citation: Full citation (author, year, title)\n",
    "2. publication_type: [Journal article/Preprint/Report/Book chapter]\n",
    "3. gcr_types: Types of catastrophic risks addressed [Nuclear/Volcanic/Asteroid/Infrastructure/Pandemic/Climate/Multiple]\n",
    "4. geographic_focus: [Global/Regional/National/Local/Islands - specify]\n",
    "5. geographic_factors: List key geographic factors (location, climate, resources, etc.)\n",
    "6. institutional_factors: List key institutional factors (governance, policies, social systems, etc.)\n",
    "7. infrastructural_factors: List key infrastructure factors (energy, food, communications, etc.)\n",
    "8. other_resilience_factors: Any resilience factors not fitting above categories\n",
    "9. study_approach: [Model/Empirical/Review/Case study/Theoretical]\n",
    "10. resilience_phase: [Preparedness/Robustness/Recovery/Adaptation]\n",
    "11. main_resilience_factors: Brief summary of main resilience-enhancing factors\n",
    "12. resilience_tradeoffs: [Yes/No] with description of any identified trade-offs\n",
    "13. vulnerable_resilient_regions: List of particularly vulnerable or resilient regions identified\n",
    "14. overall_relevance: [Low/Medium/High] relevance to our research question\n",
    "15. evidence_gaps: Brief description of critical missing validation elements\n",
    "\n",
    "CRITICAL CSV FORMATTING REQUIREMENTS:\n",
    "\n",
    "- Wrap ALL text fields in double quotes, even if they don't contain commas\n",
    "- If a text field contains double quotes, escape them by doubling them (\"\")\n",
    "- Use commas ONLY as field separators between fields\n",
    "- Do not use any quotes within the field content except for properly escaped ones\n",
    "- Each field must be enclosed in double quotes: \"field content\"\n",
    "\n",
    "Example format: \"text field 1\",\"text field 2\",\"text field 3\"\n",
    "The entire row should look like: \"field1\",\"field2\",\"field3\",...,\"field15\"\n",
    "\n",
    "For fields with multiple options, use the exact values specified in brackets. Please analyze the paper thoroughly before extracting the information.\n",
    "Respond with ONLY the CSV row (no column headers, no additional text).\n",
    "\n",
    "For text fields, place the content in double quotes to properly handle any commas. For fields with multiple options, use the exact values specified in brackets. Please analyze the paper thoroughly before extracting the information. Respond with ONLY the CSV row (no column headers).\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components\n",
    "text_extractor = PDFExtractor()\n",
    "tokenizer = Tokenizer()\n",
    "llm_client = ClaudeClient(api_key)\n",
    "cache = FileCache(Path(\"prompt_cache/extraction_prompt_cache.json\"))\n",
    "storage = CSVStorage(Path(\"gcr_resilience_extraction_results.csv\"))\n",
    "\n",
    "# Create processor\n",
    "processor = PaperProcessor(\n",
    "    text_extractor=text_extractor,\n",
    "    tokenizer=tokenizer,\n",
    "    llm_client=llm_client,\n",
    "    cache=cache,\n",
    "    storage=storage\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Papers\n",
    "\n",
    "Process all papers in the PDF directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   0%|          | 0/147 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-15 19:02:43,728 - INFO - Loading cached results for: Tzachor - 2022 - A System Dynamics Perspective of Food Systems, Environmental Change and Global Catastrophic Risks.pdf\n",
      "2025-06-15 19:02:43,979 - ERROR - Error parsing response for Tzachor - 2022 - A System Dynamics Perspective of Food Systems, Environmental Change and Global Catastrophic Risks.pdf: 'PaperProcessor' object has no attribute 'current_query'\n",
      "Processing PDFs:   1%|          | 1/147 [00:00<00:37,  3.88it/s]2025-06-15 19:02:43,981 - INFO - Loading cached results for: Beckstead - 2015 - How much could refuges help us recover from a global catastrophe.pdf\n",
      "2025-06-15 19:02:44,092 - ERROR - Error parsing response for Beckstead - 2015 - How much could refuges help us recover from a global catastrophe.pdf: 'PaperProcessor' object has no attribute 'current_query'\n",
      "Processing PDFs:   1%|▏         | 2/147 [00:00<00:25,  5.79it/s]2025-06-15 19:02:44,094 - INFO - Loading cached results for: Jones et al. - 2023 - Scoping Potential Routes to UK Civil Unrest via the Food System Results of a Structured Expert Elic 05.00.57.pdf\n",
      "2025-06-15 19:02:44,227 - ERROR - Error parsing response for Jones et al. - 2023 - Scoping Potential Routes to UK Civil Unrest via the Food System Results of a Structured Expert Elic 05.00.57.pdf: 'PaperProcessor' object has no attribute 'current_query'\n",
      "Processing PDFs:   2%|▏         | 3/147 [00:00<00:22,  6.44it/s]2025-06-15 19:02:44,229 - INFO - Loading cached results for: Mani et al. - 2022 - Lower magnitude volcanic eruptions as Global Catastrophic Risks.pdf\n",
      "2025-06-15 19:02:44,266 - ERROR - Error parsing response for Mani et al. - 2022 - Lower magnitude volcanic eruptions as Global Catastrophic Risks.pdf: 'PaperProcessor' object has no attribute 'current_query'\n",
      "2025-06-15 19:02:44,267 - INFO - Loading cached results for: Doran et al. - 2024 - What can we learn from historical pandemics A systematic review of the literature.pdf\n",
      "2025-06-15 19:02:44,909 - ERROR - Error parsing response for Doran et al. - 2024 - What can we learn from historical pandemics A systematic review of the literature.pdf: 'PaperProcessor' object has no attribute 'current_query'\n",
      "Processing PDFs:   3%|▎         | 5/147 [00:01<00:37,  3.84it/s]2025-06-15 19:02:44,911 - INFO - Loading cached results for: Räisänen et al. - 2023 - Imagining the next pandemic Finnish preparedness for chronic transboundary crises before and during.pdf\n",
      "2025-06-15 19:02:45,059 - ERROR - Error parsing response for Räisänen et al. - 2023 - Imagining the next pandemic Finnish preparedness for chronic transboundary crises before and during.pdf: 'PaperProcessor' object has no attribute 'current_query'\n",
      "Processing PDFs:   4%|▍         | 6/147 [00:01<00:32,  4.37it/s]2025-06-15 19:02:45,061 - INFO - Loading cached results for: Cockell and Stokes - 1999 - [No title found].pdf\n",
      "2025-06-15 19:02:45,175 - ERROR - Error parsing response for Cockell and Stokes - 1999 - [No title found].pdf: 'PaperProcessor' object has no attribute 'current_query'\n",
      "Processing PDFs:   5%|▍         | 7/147 [00:01<00:27,  5.10it/s]2025-06-15 19:02:45,177 - INFO - Loading cached results for: Baum - 2018 - Uncertain human consequences in asteroid risk analysis and the global catastrophe threshold.pdf\n",
      "2025-06-15 19:02:45,360 - ERROR - Error parsing response for Baum - 2018 - Uncertain human consequences in asteroid risk analysis and the global catastrophe threshold.pdf: 'PaperProcessor' object has no attribute 'current_query'\n",
      "Processing PDFs:   5%|▌         | 8/147 [00:01<00:26,  5.19it/s]2025-06-15 19:02:45,362 - INFO - Loading cached results for: Ledesma et al. - 2023 - Evaluation of the Global Health Security Index as a predictor of COVID-19 excess mortality standardi.pdf\n",
      "2025-06-15 19:02:45,490 - ERROR - Error parsing response for Ledesma et al. - 2023 - Evaluation of the Global Health Security Index as a predictor of COVID-19 excess mortality standardi.pdf: 'PaperProcessor' object has no attribute 'current_query'\n",
      "Processing PDFs:   6%|▌         | 9/147 [00:01<00:24,  5.74it/s]2025-06-15 19:02:45,492 - INFO - Loading cached results for: 2022 - Lessons Learned from the COVID-19 Outbreak Preventing and Managing Future Pandemics.pdf\n",
      "2025-06-15 19:02:45,496 - ERROR - Error processing 2022 - Lessons Learned from the COVID-19 Outbreak Preventing and Managing Future Pandemics.pdf: PyCryptodome is required for AES algorithm\n",
      "2025-06-15 19:02:45,498 - WARNING - File Cooper and Sovacool - 2011 - Not Your Father's Y2K Preparing the North American Power Grid for the Perfect Solar Storm.pdf marked as processed but no cache found, reprocessing...\n",
      "2025-06-15 19:02:45,498 - INFO - Processing Cooper and Sovacool - 2011 - Not Your Father's Y2K Preparing the North American Power Grid for the Perfect Solar Storm.pdf...\n",
      "2025-06-15 19:02:45,565 - INFO - Extracted 52893 characters from Cooper and Sovacool - 2011 - Not Your Father's Y2K Preparing the North American Power Grid for the Perfect Solar Storm.pdf\n",
      "2025-06-15 19:02:45,566 - INFO - Using cached response for Cooper and Sovacool - 2011 - Not Your Father's Y2K Preparing the North American Power Grid for the Perfect Solar Storm.pdf\n",
      "2025-06-15 19:02:45,567 - INFO - Successfully processed Cooper and Sovacool - 2011 - Not Your Father's Y2K Preparing the North American Power Grid for the Perfect Solar Storm.pdf\n",
      "2025-06-15 19:02:45,568 - WARNING - File Luby and Arthur - 2019 - Risk and Response to Biological Catastrophe in Lower Income Countries.pdf marked as processed but no cache found, reprocessing...\n",
      "2025-06-15 19:02:45,568 - INFO - Processing Luby and Arthur - 2019 - Risk and Response to Biological Catastrophe in Lower Income Countries.pdf...\n",
      "2025-06-15 19:02:45,684 - INFO - Extracted 67904 characters from Luby and Arthur - 2019 - Risk and Response to Biological Catastrophe in Lower Income Countries.pdf\n",
      "2025-06-15 19:02:45,685 - INFO - Using cached response for Luby and Arthur - 2019 - Risk and Response to Biological Catastrophe in Lower Income Countries.pdf\n",
      "2025-06-15 19:02:45,686 - INFO - Successfully processed Luby and Arthur - 2019 - Risk and Response to Biological Catastrophe in Lower Income Countries.pdf\n",
      "Processing PDFs:   8%|▊         | 12/147 [00:01<00:15,  8.83it/s]2025-06-15 19:02:45,688 - WARNING - File Tonn and Stiefel - 2014 - Human extinction risk and uncertainty Assessing conditions for action.pdf marked as processed but no cache found, reprocessing...\n",
      "2025-06-15 19:02:45,688 - INFO - Processing Tonn and Stiefel - 2014 - Human extinction risk and uncertainty Assessing conditions for action.pdf...\n",
      "2025-06-15 19:02:45,889 - INFO - Extracted 42523 characters from Tonn and Stiefel - 2014 - Human extinction risk and uncertainty Assessing conditions for action.pdf\n",
      "2025-06-15 19:02:53,157 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-06-15 19:02:53,178 - INFO - Successfully processed Tonn and Stiefel - 2014 - Human extinction risk and uncertainty Assessing conditions for action.pdf\n",
      "Processing PDFs:   9%|▉         | 13/147 [00:09<03:43,  1.66s/it]2025-06-15 19:02:53,182 - WARNING - File Neogi et al. - 2021 - The predictors of COVID-19 mortality among health systems and Global Health Security parameters An.pdf marked as processed but no cache found, reprocessing...\n",
      "2025-06-15 19:02:53,183 - INFO - Processing Neogi et al. - 2021 - The predictors of COVID-19 mortality among health systems and Global Health Security parameters An.pdf...\n",
      "2025-06-15 19:02:53,885 - INFO - Extracted 31440 characters from Neogi et al. - 2021 - The predictors of COVID-19 mortality among health systems and Global Health Security parameters An.pdf\n",
      "2025-06-15 19:02:53,889 - INFO - Rate limit reached. Waiting 59.3 seconds...\n",
      "Processing PDFs:   9%|▉         | 13/147 [00:30<05:14,  2.35s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Process papers\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m processor\u001b[38;5;241m.\u001b[39mprocess_directory(Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpdf\u001b[39m\u001b[38;5;124m\"\u001b[39m), query)\n",
      "Cell \u001b[0;32mIn[6], line 192\u001b[0m, in \u001b[0;36mPaperProcessor.process_directory\u001b[0;34m(self, directory, query)\u001b[0m\n\u001b[1;32m    189\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdf_path\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m marked as processed but no cache found, reprocessing...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    191\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdf_path\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 192\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_paper(pdf_path, query)\n\u001b[1;32m    193\u001b[0m results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# Save after each successful processing\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 98\u001b[0m, in \u001b[0;36mPaperProcessor.process_paper\u001b[0;34m(self, file_path, query)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# Check rate limit and wait if necessary\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     total_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mcount_tokens(current_text) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_tokens \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mcount_tokens(query)\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_rate_limit(total_tokens)\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# Try to process\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_client\u001b[38;5;241m.\u001b[39mprocess_text(current_text, query)\n",
      "Cell \u001b[0;32mIn[6], line 70\u001b[0m, in \u001b[0;36mPaperProcessor._wait_for_rate_limit\u001b[0;34m(self, required_tokens)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait_time \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     69\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRate limit reached. Waiting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwait_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 70\u001b[0m     sleep_time\u001b[38;5;241m.\u001b[39msleep(wait_time)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cleanup_old_usage()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Process papers\n",
    "processor.process_directory(Path(\"pdf\"), query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
