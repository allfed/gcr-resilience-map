{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Processing System\n",
    "\n",
    "This notebook processes academic papers to extract information about regional resilience to catastrophic risks using Claude.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's import all required libraries and set up logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from pathlib import Path\n",
    "import PyPDF2\n",
    "import anthropic\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "import csv\n",
    "import json\n",
    "from io import StringIO\n",
    "import hashlib\n",
    "import datetime\n",
    "import time as sleep_time\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Dict, Any, Set  # Added List and other type hints\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structures and Classes\n",
    "\n",
    "Define the data structures and classes needed for processing papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PaperMetadata:\n",
    "    \"\"\"Data class for paper metadata\"\"\"\n",
    "    filename: str\n",
    "    paper_citation: Optional[str] = None\n",
    "    publication_type: Optional[str] = None\n",
    "    gcr_types: Optional[str] = None\n",
    "    geographic_focus: Optional[str] = None\n",
    "    geographic_factors: Optional[str] = None\n",
    "    institutional_factors: Optional[str] = None\n",
    "    infrastructural_factors: Optional[str] = None\n",
    "    other_resilience_factors: Optional[str] = None\n",
    "    study_approach: Optional[str] = None\n",
    "    resilience_phase: Optional[str] = None\n",
    "    main_resilience_factors: Optional[str] = None\n",
    "    resilience_tradeoffs: Optional[str] = None\n",
    "    vulnerable_resilient_regions: Optional[str] = None\n",
    "    overall_relevance: Optional[str] = None\n",
    "    evidence_gaps: Optional[str] = None\n",
    "    error: Optional[str] = None\n",
    "    current_query: Optional[str] = None  # Added for token calculations\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert to dictionary for storage, excluding current_query\"\"\"\n",
    "        return {k: v for k, v in self.__dict__.items()\n",
    "                if v is not None and k != 'current_query'}  # Exclude current_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDFExtractor:\n",
    "    \"\"\"Extracts text from PDF files\"\"\"\n",
    "    def extract(self, file_path: Path) -> str:\n",
    "        \"\"\"Extract text from a PDF file\"\"\"\n",
    "        with open(file_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "        return text\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"Handles text tokenization and truncation\"\"\"\n",
    "    def __init__(self, encoding_name: str = \"cl100k_base\"):\n",
    "        self.encoding = tiktoken.get_encoding(encoding_name)\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Count tokens in text\"\"\"\n",
    "        return len(self.encoding.encode(text))\n",
    "    \n",
    "    def truncate(self, text: str, ratio: float) -> str:\n",
    "        \"\"\"Truncate text to a given ratio of its original length\"\"\"\n",
    "        tokens = self.encoding.encode(text)\n",
    "        return self.encoding.decode(tokens[:int(len(tokens) * ratio)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClaudeClient:\n",
    "    \"\"\"Client for Claude API\"\"\"\n",
    "    def __init__(self, api_key: str, model: str = \"claude-3-7-sonnet-20250219\"):\n",
    "        self.client = anthropic.Anthropic(api_key=api_key)\n",
    "        self.model = model\n",
    "    \n",
    "    def process_text(self, text: str, query: str, **kwargs) -> str:\n",
    "        \"\"\"Process text with Claude\"\"\"\n",
    "        response = self.client.messages.create(\n",
    "            model=self.model,\n",
    "            max_tokens=kwargs.get('max_tokens', 1000),\n",
    "            temperature=kwargs.get('temperature', 0),\n",
    "            system=[{\"type\": \"text\", \"text\": \"You are an AI assistant tasked with analyzing documents.\"}],\n",
    "            messages=[{\"role\": \"user\", \"content\": f\"Document content:\\n{text}\\n\\n{query}\"}]\n",
    "        )\n",
    "        return response.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileCache:\n",
    "    \"\"\"Simple file-based cache\"\"\"\n",
    "    def __init__(self, cache_file: Path):\n",
    "        self.cache_file = cache_file\n",
    "        self.cache: Dict[str, str] = {}\n",
    "        self._load_cache()\n",
    "    \n",
    "    def _load_cache(self) -> None:\n",
    "        \"\"\"Load cache from file\"\"\"\n",
    "        if self.cache_file.exists():\n",
    "            with open(self.cache_file, 'r') as f:\n",
    "                self.cache = json.load(f)\n",
    "    \n",
    "    def _save_cache(self) -> None:\n",
    "        \"\"\"Save cache to file\"\"\"\n",
    "        with open(self.cache_file, 'w') as f:\n",
    "            json.dump(self.cache, f)\n",
    "    \n",
    "    def get(self, key: str) -> Optional[str]:\n",
    "        \"\"\"Get cached response\"\"\"\n",
    "        return self.cache.get(key)\n",
    "    \n",
    "    def set(self, key: str, value: str) -> None:\n",
    "        \"\"\"Cache a response\"\"\"\n",
    "        self.cache[key] = value\n",
    "        self._save_cache()\n",
    "\n",
    "\n",
    "class CSVStorage:\n",
    "    \"\"\"Handles storage of results in CSV format\"\"\"\n",
    "\n",
    "    def __init__(self, output_file: Path):\n",
    "        self.output_file = output_file\n",
    "        self._ensure_file_exists()\n",
    "\n",
    "    def _ensure_file_exists(self):\n",
    "        if not self.output_file.exists():\n",
    "            # Create empty file with headers\n",
    "            df = pd.DataFrame(columns=PaperMetadata.__annotations__.keys())\n",
    "            df.to_csv(self.output_file, index=False)\n",
    "\n",
    "    def is_processed(self, filename: str) -> bool:\n",
    "        \"\"\"Check if a file has already been processed\"\"\"\n",
    "        try:\n",
    "            if not self.output_file.exists():\n",
    "                return False\n",
    "            df = pd.read_csv(self.output_file)\n",
    "            return filename in df['filename'].values\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error checking if file is processed: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def get_processed_files(self) -> set:\n",
    "        \"\"\"Get set of all processed filenames\"\"\"\n",
    "        try:\n",
    "            if not self.output_file.exists():\n",
    "                return set()\n",
    "            df = pd.read_csv(self.output_file)\n",
    "            return set(df['filename'].values)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting processed files: {str(e)}\")\n",
    "            return set()\n",
    "\n",
    "    def save_results(self, results: List[PaperMetadata]):\n",
    "        \"\"\"Save results to CSV file\"\"\"\n",
    "        try:\n",
    "            df = pd.DataFrame([r.to_dict() for r in results])\n",
    "            df.to_csv(self.output_file, index=False)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving results: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperProcessor:\n",
    "    \"\"\"Main paper processing class\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_extractor: PDFExtractor,\n",
    "        tokenizer: Tokenizer,\n",
    "        llm_client: ClaudeClient,\n",
    "        cache: FileCache,\n",
    "        storage: CSVStorage,\n",
    "        max_tokens: int = 4000,\n",
    "        truncation_ratio: float = 0.8,\n",
    "        tokens_per_minute: int = 20000,  # Claude's rate limit\n",
    "        max_total_tokens: int = 200000   # Claude's total token limit\n",
    "    ):\n",
    "        self.text_extractor = text_extractor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.llm_client = llm_client\n",
    "        self.cache = cache\n",
    "        self.storage = storage\n",
    "        self.max_tokens = max_tokens\n",
    "        self.truncation_ratio = truncation_ratio\n",
    "        self.tokens_per_minute = tokens_per_minute\n",
    "        self.max_total_tokens = max_total_tokens\n",
    "\n",
    "        # Rate limiting tracking\n",
    "        self.token_usage = []  # List of (timestamp, token_count) tuples\n",
    "\n",
    "        # Calculate prompt tokens once\n",
    "        self.prompt_tokens = self.tokenizer.count_tokens(\n",
    "            \"Document content:\\n\\n\" +  # Base prompt\n",
    "            \"You are an AI assistant tasked with analyzing documents.\"  # System prompt\n",
    "        )\n",
    "\n",
    "    def _calculate_max_paper_tokens(self) -> int:\n",
    "        \"\"\"Calculate maximum tokens allowed for the paper text\"\"\"\n",
    "        # Reserve space for the query and other prompt elements\n",
    "        query_tokens = self.tokenizer.count_tokens(self.current_query)\n",
    "        reserved_tokens = self.prompt_tokens + query_tokens + 1000  # Add buffer\n",
    "        return self.max_total_tokens - reserved_tokens\n",
    "\n",
    "    def _cleanup_old_usage(self):\n",
    "        \"\"\"Remove token usage records older than 1 minute\"\"\"\n",
    "        current_time = datetime.datetime.now()\n",
    "        one_minute_ago = current_time - datetime.timedelta(minutes=1)\n",
    "        self.token_usage = [(ts, count) for ts, count in self.token_usage if ts > one_minute_ago]\n",
    "\n",
    "    def _get_current_token_usage(self) -> int:\n",
    "        \"\"\"Get total token usage in the last minute\"\"\"\n",
    "        self._cleanup_old_usage()\n",
    "        return sum(count for _, count in self.token_usage)\n",
    "\n",
    "    def _wait_for_rate_limit(self, required_tokens: int):\n",
    "        \"\"\"Wait if necessary to stay within rate limits\"\"\"\n",
    "        while True:\n",
    "            current_usage = self._get_current_token_usage()\n",
    "            if current_usage + required_tokens <= self.tokens_per_minute:\n",
    "                break\n",
    "            \n",
    "            # Check if we have any usage records\n",
    "            if not self.token_usage:\n",
    "                # If no records, we can proceed\n",
    "                break\n",
    "                \n",
    "            # Calculate how long to wait\n",
    "            oldest_record = min(ts for ts, _ in self.token_usage)\n",
    "            wait_time = 60 - (datetime.datetime.now() - oldest_record).total_seconds()\n",
    "            if wait_time > 0:\n",
    "                logger.info(f\"Rate limit reached. Waiting {wait_time:.1f} seconds...\")\n",
    "                sleep_time.sleep(wait_time)\n",
    "            self._cleanup_old_usage()\n",
    "\n",
    "    def _record_token_usage(self, token_count: int):\n",
    "        \"\"\"Record token usage for rate limiting\"\"\"\n",
    "        self.token_usage.append((datetime.datetime.now(), token_count))\n",
    "\n",
    "    def process_paper(self, file_path: Path, query: str) -> PaperMetadata:\n",
    "        \"\"\"Process a single paper\"\"\"\n",
    "        # Store query for token calculations\n",
    "        self.current_query = query\n",
    "\n",
    "        # Extract text\n",
    "        text = self.text_extractor.extract(file_path)\n",
    "        logger.info(f\"Extracted {len(text)} characters from {file_path.name}\")\n",
    "\n",
    "        # Check cache\n",
    "        cache_key = hashlib.md5((text + query).encode()).hexdigest()\n",
    "        if cached_response := self.cache.get(cache_key):\n",
    "            logger.info(f\"Using cached response for {file_path.name}\")\n",
    "            return self._parse_response(cached_response, file_path.name)\n",
    "\n",
    "        # Process with LLM\n",
    "        current_text = text\n",
    "        while True:\n",
    "            try:\n",
    "                # Check rate limit and wait if necessary\n",
    "                total_tokens = self.tokenizer.count_tokens(current_text) + self.prompt_tokens + self.tokenizer.count_tokens(query)\n",
    "                self._wait_for_rate_limit(total_tokens)\n",
    "                \n",
    "                # Try to process\n",
    "                response = self.llm_client.process_text(current_text, query)\n",
    "                self._record_token_usage(total_tokens)\n",
    "                self.cache.set(cache_key, response)\n",
    "                return self._parse_response(response, file_path.name)\n",
    "                \n",
    "            except Exception as e:\n",
    "                if \"too long\" in str(e).lower():\n",
    "                    # Truncate to 80% of current length\n",
    "                    current_tokens = self.tokenizer.count_tokens(current_text)\n",
    "                    target_tokens = int(current_tokens * self.truncation_ratio)\n",
    "                    target_tokens = max(1000, target_tokens)  # Ensure minimum length\n",
    "                    \n",
    "                    logger.info(f\"Token limit exceeded. Truncating from {current_tokens} to {target_tokens} tokens...\")\n",
    "                    tokens = self.tokenizer.encoding.encode(current_text)\n",
    "                    current_text = self.tokenizer.encoding.decode(tokens[:target_tokens])\n",
    "                else:\n",
    "                    raise e\n",
    "                \n",
    "    def _parse_response(self, response: str, filename: str) -> PaperMetadata:\n",
    "        \"\"\"Parse CSV response into PaperMetadata object\"\"\"\n",
    "        try:\n",
    "            # Clean the response text\n",
    "            clean_text = response.strip()\n",
    "            \n",
    "            # Find the first line that looks like a CSV row (has multiple commas)\n",
    "            # and doesn't contain the prompt text\n",
    "            lines = clean_text.split('\\n')\n",
    "            csv_line = None\n",
    "            for line in lines:\n",
    "                # Skip lines that contain parts of the prompt\n",
    "                if \"research question\" in line.lower() or \"csv format\" in line.lower():\n",
    "                    continue\n",
    "                # Find the line with the most commas that's not the prompt\n",
    "                if line.count(',') >= 10:  # We expect at least 10 commas for our CSV format\n",
    "                    csv_line = line\n",
    "                    break\n",
    "            \n",
    "            if not csv_line:\n",
    "                raise ValueError(\"No valid CSV line found in response\")\n",
    "            \n",
    "            # Parse CSV\n",
    "            reader = csv.reader(StringIO(csv_line))\n",
    "            row = next(reader)\n",
    "            \n",
    "            # Create PaperMetadata object with all fields\n",
    "            return PaperMetadata(\n",
    "                filename=filename,\n",
    "                paper_citation=row[0] if len(row) > 0 else None,\n",
    "                publication_type=row[1] if len(row) > 1 else None,\n",
    "                gcr_types=row[2] if len(row) > 2 else None,\n",
    "                geographic_focus=row[3] if len(row) > 3 else None,\n",
    "                geographic_factors=row[4] if len(row) > 4 else None,\n",
    "                institutional_factors=row[5] if len(row) > 5 else None,\n",
    "                infrastructural_factors=row[6] if len(row) > 6 else None,\n",
    "                other_resilience_factors=row[7] if len(row) > 7 else None,\n",
    "                study_approach=row[8] if len(row) > 8 else None,\n",
    "                resilience_phase=row[9] if len(row) > 9 else None,\n",
    "                main_resilience_factors=row[10] if len(row) > 10 else None,\n",
    "                resilience_tradeoffs=row[11] if len(row) > 11 else None,\n",
    "                vulnerable_resilient_regions=row[12] if len(row) > 12 else None,\n",
    "                overall_relevance=row[13] if len(row) > 13 else None,\n",
    "                evidence_gaps=row[14] if len(row) > 14 else None,\n",
    "                current_query=self.current_query\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing response for {filename}: {str(e)}\")\n",
    "            return PaperMetadata(filename=filename, error=str(e))\n",
    "                \n",
    "    def process_directory(self, directory: Path, query: str) -> None:\n",
    "        \"\"\"Process all PDF files in a directory\"\"\"\n",
    "        results = []\n",
    "        pdf_files = list(directory.glob(\"*.pdf\"))\n",
    "        \n",
    "        for pdf_path in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "            try:\n",
    "                # Skip if already processed\n",
    "                if self.storage.is_processed(pdf_path.name):\n",
    "                    logger.info(f\"Skipping already processed file: {pdf_path.name}\")\n",
    "                    continue\n",
    "\n",
    "                logger.info(f\"Processing {pdf_path.name}...\")\n",
    "                result = self.process_paper(pdf_path, query)\n",
    "                results.append(result)\n",
    "\n",
    "                # Save after each successful processing\n",
    "                self.storage.save_results(results)\n",
    "                logger.info(f\"Successfully processed {pdf_path.name}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {pdf_path.name}: {str(e)}\")\n",
    "                # Add error record\n",
    "                error_result = PaperMetadata(\n",
    "                    filename=pdf_path.name,\n",
    "                    error=str(e)\n",
    "                )\n",
    "                results.append(error_result)\n",
    "                # Save even after errors\n",
    "                self.storage.save_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up the configuration and initialize components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key\n",
    "with open(\"../config/api_key.txt\", 'r') as f:\n",
    "    api_key = f.read().strip()\n",
    "\n",
    "# Define extraction query\n",
    "query = \"\"\"I need you to analyze the provided research paper and extract specific information about regional resilience to catastrophic risks. Our research question is: \"What specific geographical, institutional, and infrastructural factors have been empirically or theoretically identified as enhancing regional resilience to nuclear winter, large magnitude volcanic eruptions, extreme pandemics, and infrastructure collapse catastrophes, and how do these resilience factors vary across catastrophe types?\"\n",
    "\n",
    "After analyzing the paper thoroughly, provide your output in a single row CSV format with the following structure:\n",
    "\n",
    "1. paper_citation: Full citation (author, year, title)\n",
    "2. publication_type: [Journal article/Preprint/Report/Book chapter]\n",
    "3. gcr_types: Types of catastrophic risks addressed [Nuclear/Volcanic/Asteroid/Infrastructure/Pandemic/Climate/Multiple]\n",
    "4. geographic_focus: [Global/Regional/National/Local/Islands - specify]\n",
    "5. geographic_factors: List key geographic factors (location, climate, resources, etc.)\n",
    "6. institutional_factors: List key institutional factors (governance, policies, social systems, etc.)\n",
    "7. infrastructural_factors: List key infrastructure factors (energy, food, communications, etc.)\n",
    "8. other_resilience_factors: Any resilience factors not fitting above categories\n",
    "9. study_approach: [Model/Empirical/Review/Case study/Theoretical]\n",
    "10. resilience_phase: [Preparedness/Robustness/Recovery/Adaptation]\n",
    "11. main_resilience_factors: Brief summary of main resilience-enhancing factors\n",
    "12. resilience_tradeoffs: [Yes/No] with description of any identified trade-offs\n",
    "13. vulnerable_resilient_regions: List of particularly vulnerable or resilient regions identified\n",
    "14. overall_relevance: [Low/Medium/High] relevance to our research question\n",
    "15. evidence_gaps: Brief description of critical missing validation elements\n",
    "\n",
    "CRITICAL CSV FORMATTING REQUIREMENTS:\n",
    "\n",
    "- Wrap ALL text fields in double quotes, even if they don't contain commas\n",
    "- If a text field contains double quotes, escape them by doubling them (\"\")\n",
    "- Use commas ONLY as field separators between fields\n",
    "- Do not use any quotes within the field content except for properly escaped ones\n",
    "- Each field must be enclosed in double quotes: \"field content\"\n",
    "\n",
    "Example format: \"text field 1\",\"text field 2\",\"text field 3\"\n",
    "The entire row should look like: \"field1\",\"field2\",\"field3\",...,\"field15\"\n",
    "\n",
    "For fields with multiple options, use the exact values specified in brackets. Please analyze the paper thoroughly before extracting the information.\n",
    "Respond with ONLY the CSV row (no column headers, no additional text).\n",
    "\n",
    "For text fields, place the content in double quotes to properly handle any commas. For fields with multiple options, use the exact values specified in brackets. Please analyze the paper thoroughly before extracting the information. Respond with ONLY the CSV row (no column headers).\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components\n",
    "text_extractor = PDFExtractor()\n",
    "tokenizer = Tokenizer()\n",
    "llm_client = ClaudeClient(api_key)\n",
    "cache = FileCache(Path(\"prompt_cache/extraction_prompt_cache.json\"))\n",
    "storage = CSVStorage(Path(\"gcr_resilience_extraction_results.csv\"))\n",
    "\n",
    "# Create processor\n",
    "processor = PaperProcessor(\n",
    "    text_extractor=text_extractor,\n",
    "    tokenizer=tokenizer,\n",
    "    llm_client=llm_client,\n",
    "    cache=cache,\n",
    "    storage=storage\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Papers\n",
    "\n",
    "Process all papers in the PDF directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   0%|          | 0/22 [00:00<?, ?it/s]2025-06-09 04:12:32,681 - INFO - Skipping already processed file: Moersdorf et al. - 2024 - The Fragile State of Industrial Agriculture Estimating Crop Yield Reductions in a Global Catastroph.pdf\n",
      "2025-06-09 04:12:32,682 - INFO - Processing Boyd et al. - 2024 - Mitigating imported fuel dependency in agricultural production Case study of an island nation's vul.pdf...\n",
      "2025-06-09 04:12:33,768 - INFO - Extracted 72506 characters from Boyd et al. - 2024 - Mitigating imported fuel dependency in agricultural production Case study of an island nation's vul.pdf\n",
      "2025-06-09 04:12:48,782 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-06-09 04:12:48,807 - INFO - Successfully processed Boyd et al. - 2024 - Mitigating imported fuel dependency in agricultural production Case study of an island nation's vul.pdf\n",
      "Processing PDFs:   9%|▉         | 2/22 [00:16<02:41,  8.07s/it]2025-06-09 04:12:48,811 - INFO - Processing Beggan et al. - 2018 - The ground effects of severe space weather.pdf...\n",
      "2025-06-09 04:12:48,913 - INFO - Extracted 26135 characters from Beggan et al. - 2018 - The ground effects of severe space weather.pdf\n",
      "2025-06-09 04:12:48,916 - INFO - Rate limit reached. Waiting 59.9 seconds...\n",
      "2025-06-09 04:14:01,564 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-06-09 04:14:01,573 - INFO - Successfully processed Beggan et al. - 2018 - The ground effects of severe space weather.pdf\n",
      "Processing PDFs:  14%|█▎        | 3/22 [01:28<11:05, 35.02s/it]2025-06-09 04:14:01,578 - INFO - Processing Rivers et al. - 2022 - Deployment of Resilient Foods Can Greatly Reduce Faminein an Abrupt Sunlight Reduction Scenario.pdf...\n",
      "2025-06-09 04:14:01,938 - INFO - Extracted 60834 characters from Rivers et al. - 2022 - Deployment of Resilient Foods Can Greatly Reduce Faminein an Abrupt Sunlight Reduction Scenario.pdf\n",
      "2025-06-09 04:14:01,944 - INFO - Rate limit reached. Waiting 59.6 seconds...\n",
      "Processing PDFs:  14%|█▎        | 3/22 [01:45<11:05, 35.03s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Process papers\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m processor\u001b[38;5;241m.\u001b[39mprocess_directory(Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpdf\u001b[39m\u001b[38;5;124m\"\u001b[39m), query)\n",
      "Cell \u001b[0;32mIn[15], line 182\u001b[0m, in \u001b[0;36mPaperProcessor.process_directory\u001b[0;34m(self, directory, query)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    181\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdf_path\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 182\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_paper(pdf_path, query)\n\u001b[1;32m    183\u001b[0m results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# Save after each successful processing\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 98\u001b[0m, in \u001b[0;36mPaperProcessor.process_paper\u001b[0;34m(self, file_path, query)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# Check rate limit and wait if necessary\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     total_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mcount_tokens(current_text) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_tokens \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mcount_tokens(query)\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_rate_limit(total_tokens)\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# Try to process\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_client\u001b[38;5;241m.\u001b[39mprocess_text(current_text, query)\n",
      "Cell \u001b[0;32mIn[15], line 70\u001b[0m, in \u001b[0;36mPaperProcessor._wait_for_rate_limit\u001b[0;34m(self, required_tokens)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait_time \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     69\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRate limit reached. Waiting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwait_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 70\u001b[0m     sleep_time\u001b[38;5;241m.\u001b[39msleep(wait_time)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cleanup_old_usage()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Process papers\n",
    "processor.process_directory(Path(\"pdf\"), query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
