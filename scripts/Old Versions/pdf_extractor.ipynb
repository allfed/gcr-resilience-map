{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Information Extractor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import PyPDF2\n",
    "import anthropic\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "import re\n",
    "import csv\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Anthropic Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Key\n",
    "api_key = 'HAHA, SURELY NOT ON A PUBLIC REPOSITORY'\n",
    "\n",
    "# Initialize Anthropic client\n",
    "client = anthropic.Anthropic(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF Processing Functions\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extract text from a PDF file\"\"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def process_with_claude(text: str, query: str, temperature: float = 0, max_tokens: int = 1000) -> str:\n",
    "    \"\"\"Process text with Claude model\"\"\"\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    token_count = len(encoding.encode(text))\n",
    "    token_count = int(token_count * 1.1)\n",
    "    print(f\"Token count is approximately {token_count}\")\n",
    "\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-7-sonnet-20250219\",\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"{query}\\n\\nPaper content:\\n{text}\"\n",
    "        }]\n",
    "    )\n",
    "    return response.content[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"I need you to analyze the following research paper and extract specific information about regional resilience to catastrophic risks. Our research question is: \"What specific geographical, institutional, and infrastructural factors have been empirically or theoretically identified as enhancing regional resilience to nuclear winter, large magnitude volcanic eruptions, extreme pandemics, and infrastructure collapse catastrophes, and how do these resilience factors vary across catastrophe types?\"\n",
    "\n",
    "After analyzing the paper thoroughly, provide your output in a single row CSV format with the following structure:\n",
    "\n",
    "1. paper_citation: Full citation (author, year, title)\n",
    "2. publication_type: [Journal article/Preprint/Report/Book chapter]\n",
    "3. gcr_types: Types of catastrophic risks addressed [Nuclear/Volcanic/Asteroid/Infrastructure/Pandemic/Climate/Multiple]\n",
    "4. geographic_focus: [Global/Regional/National/Local/Islands - specify]\n",
    "5. regions_compared: [Yes/No] with brief description if yes\n",
    "6. geographic_factors: List key geographic factors (location, climate, resources, etc.)\n",
    "7. institutional_factors: List key institutional factors (governance, policies, social systems, etc.)\n",
    "8. infrastructural_factors: List key infrastructure factors (energy, food, communications, etc.)\n",
    "9. other_resilience_factors: Any resilience factors not fitting above categories\n",
    "10. study_approach: [Model/Empirical/Review/Case study/Theoretical]\n",
    "11. evidence_strength: [Low/Medium/High] with brief justification\n",
    "12. evidence_causal: [TRUE/FALSE] for direct causal evidence\n",
    "13. evidence_predictive: [TRUE/FALSE] for predictive evidence\n",
    "14. evidence_correlational: [TRUE/FALSE] for correlational evidence\n",
    "15. evidence_theoretical: [TRUE/FALSE] for theoretical/expert opinion\n",
    "16. evidence_case_study: [TRUE/FALSE] for case study observations\n",
    "17. evidence_model: [TRUE/FALSE] for model-based projections\n",
    "18. validation_external: [TRUE/FALSE] for external validation against outcomes\n",
    "19. validation_alternative: [TRUE/FALSE] for validation against alternative datasets\n",
    "20. validation_temporal: [TRUE/FALSE] for validation across multiple time periods\n",
    "21. validation_cross_regional: [TRUE/FALSE] for cross-regional validation\n",
    "22. validation_none: [TRUE/FALSE] if no validation attempted\n",
    "23. counterfactual_robust: [TRUE/FALSE] for robust counterfactual analysis\n",
    "24. counterfactual_limited: [TRUE/FALSE] for limited counterfactual discussion\n",
    "25. counterfactual_none: [TRUE/FALSE] if no counterfactuals considered\n",
    "26. limitations_thorough: [TRUE/FALSE] if authors thoroughly address limitations\n",
    "27. limitations_limited: [TRUE/FALSE] if limited discussion of limitations\n",
    "28. limitations_none: [TRUE/FALSE] if no significant discussion of limitations\n",
    "29. confidence_assessment: [High/Medium/Low/Very low] for key resilience factors\n",
    "30. evidence_gaps: Brief description of critical missing validation elements\n",
    "31. resilience_phase: [Preparedness/Robustness/Recovery/Adaptation]\n",
    "32. implemented_measures: Brief description of measures already implemented (if any)\n",
    "33. proposed_measures: Brief description of proposed measures (if any)\n",
    "34. main_resilience_factors: Brief summary of main resilience-enhancing factors\n",
    "35. differential_effectiveness: [Yes/No] with description if factors vary across GCR types\n",
    "36. resilience_tradeoffs: [Yes/No] with description of any identified trade-offs\n",
    "37. vulnerable_resilient_regions: List of particularly vulnerable or resilient regions identified\n",
    "38. overall_relevance: [Low/Medium/High] relevance to our research question\n",
    "39. key_quotes: 1-2 most relevant quotes supporting findings\n",
    "40. additional_notes: Any other important observations\n",
    "\n",
    "For text fields, place the content in double quotes to properly handle any commas. For boolean fields, use TRUE or FALSE. For fields with multiple options like evidence types, mark TRUE for all that apply.\n",
    "\n",
    "Please analyze the paper thoroughly before extracting the information. Respond with ONLY the CSV row (no column headers).\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "from datetime import datetime\n",
    "pdf_dir = \"pdf\"  # Directory containing PDF files\n",
    "temperature = 0 # keep it as deterministic as possible\n",
    "max_tokens = 4000\n",
    "output_csv = \"extraction_results.csv\"\n",
    "\n",
    "# Process PDFs\n",
    "results = []\n",
    "pdf_files = list(Path(pdf_dir).glob('*.pdf'))\n",
    "\n",
    "for pdf_path in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "    try:\n",
    "        text = extract_text_from_pdf(str(pdf_path))\n",
    "        response = \"test\"#process_with_claude(text, query, temperature, max_tokens)\n",
    "        \n",
    "        results.append(response)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_path.name}: {str(e)}\")\n",
    "\n",
    "# Save results to CSV\n",
    "print(results)\n",
    "# Define the column names based on the query structure\n",
    "columns = [\n",
    "    \"paper_citation\", \"publication_type\", \"gcr_types\", \"geographic_focus\",\n",
    "    \"regions_compared\", \"geographic_factors\", \"institutional_factors\", \"infrastructural_factors\",\n",
    "    \"other_resilience_factors\", \"study_approach\", \"evidence_strength\",\n",
    "    \"evidence_causal\", \"evidence_predictive\", \"evidence_correlational\",\n",
    "    \"evidence_theoretical\", \"evidence_case_study\", \"evidence_model\",\n",
    "    \"validation_external\", \"validation_alternative\", \"validation_temporal\",\n",
    "    \"validation_cross_regional\", \"validation_none\", \"counterfactual_robust\",\n",
    "    \"counterfactual_limited\", \"counterfactual_none\", \"limitations_thorough\",\n",
    "    \"limitations_limited\", \"limitations_none\", \"confidence_assessment\",\n",
    "    \"evidence_gaps\", \"resilience_phase\", \"implemented_measures\",\n",
    "    \"proposed_measures\", \"main_resilience_factors\", \"differential_effectiveness\",\n",
    "    \"resilience_tradeoffs\", \"vulnerable_resilient_regions\", \"overall_relevance\",\n",
    "    \"key_quotes\", \"additional_notes\"\n",
    "]\n",
    "\n",
    "def parse_csv_response(response_text):\n",
    "    \"\"\"Parse the CSV response from Claude and return a dictionary with column names as keys\"\"\"\n",
    "    # Clean the response text\n",
    "    clean_text = response_text.strip()\n",
    "\n",
    "    # If there are multiple lines, take only the CSV line\n",
    "    if \"\\n\" in clean_text:\n",
    "        # Find the line that has the most commas (likely the CSV data)\n",
    "        lines = clean_text.split('\\n')\n",
    "        clean_text = max(lines, key=lambda x: x.count(','))\n",
    "\n",
    "    # Parse CSV using the csv module which handles quoted fields properly\n",
    "    reader = csv.reader(StringIO(clean_text))\n",
    "    try:\n",
    "        row = next(reader)\n",
    "        # Map values to column names\n",
    "        result = {col: val for col, val in zip(columns, row)}\n",
    "        return result\n",
    "    except StopIteration:\n",
    "        # If parsing fails, return the original text\n",
    "        return {\"error\": \"Failed to parse CSV response\", \"original_text\": clean_text}\n",
    "\n",
    "\n",
    "# Configuration\n",
    "pdf_dir = \"pdf\"  # Directory containing PDF files\n",
    "temperature = 0   # Keep it as deterministic as possible\n",
    "max_tokens = 1500  # Increased token limit for more detailed responses\n",
    "output_csv = \"gcr_resilience_results.csv\"\n",
    "\n",
    "# Create pdf directory if it doesn't exist\n",
    "os.makedirs(pdf_dir, exist_ok=True)\n",
    "\n",
    "# Process PDFs\n",
    "results = []\n",
    "pdf_files = list(Path(pdf_dir).glob('*.pdf'))\n",
    "\n",
    "for pdf_path in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "    try:\n",
    "        print(f\"Processing {pdf_path.name}...\")\n",
    "        text = extract_text_from_pdf(str(pdf_path))\n",
    "        print(f\"Extracted {len(text)} characters from PDF.\")\n",
    "\n",
    "        # Process with Claude\n",
    "        response = process_with_claude(text, query, temperature, max_tokens)\n",
    "        print(f\"Received response of length {len(response)}\")\n",
    "\n",
    "        # Parse the CSV response\n",
    "        parsed_result = parse_csv_response(response)\n",
    "\n",
    "        # Add the filename for reference\n",
    "        parsed_result['filename'] = pdf_path.name\n",
    "\n",
    "        # Add to results\n",
    "        results.append(parsed_result)\n",
    "        print(f\"Successfully processed {pdf_path.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_path.name}: {str(e)}\")\n",
    "        # Add error record\n",
    "        results.append({\n",
    "            \"error\": str(e),\n",
    "            \"filename\": pdf_path.name\n",
    "        })\n",
    "\n",
    "# Create DataFrame and save results\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Add timestamp to create a versioned output file\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "versioned_output_csv = f\"{os.path.splitext(output_csv)[0]}_{timestamp}.csv\"\n",
    "\n",
    "# Save both versioned and standard output\n",
    "df.to_csv(output_csv, index=False)\n",
    "df.to_csv(versioned_output_csv, index=False)\n",
    "\n",
    "print(\n",
    "    f\"Results saved to {output_csv} and versioned copy at {versioned_output_csv}\")\n",
    "\n",
    "# Display results\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
